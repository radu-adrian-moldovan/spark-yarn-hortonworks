

Container: container_e05_1447159372353_0025_01_000001 on sandbox.hortonworks.com_45454
========================================================================================
LogType:stderr
Log Upload Time:Thu Nov 12 14:43:50 +0000 2015
LogLength:36524
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/usercache/hdfs/filecache/73/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/11/12 14:43:00 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
15/11/12 14:43:01 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:02 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:02 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1447159372353_0025_000001
15/11/12 14:43:02 INFO spark.SecurityManager: Changing view acls to: yarn,hdfs
15/11/12 14:43:02 INFO spark.SecurityManager: Changing modify acls to: yarn,hdfs
15/11/12 14:43:02 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hdfs); users with modify permissions: Set(yarn, hdfs)
15/11/12 14:43:03 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
15/11/12 14:43:03 INFO yarn.ApplicationMaster: Waiting for spark context initialization
15/11/12 14:43:03 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/11/12 14:43:03 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:03 INFO spark.SparkContext: Running Spark version 1.4.1
15/11/12 14:43:03 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:03 INFO spark.SecurityManager: Changing view acls to: yarn,hdfs
15/11/12 14:43:03 INFO spark.SecurityManager: Changing modify acls to: yarn,hdfs
15/11/12 14:43:03 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hdfs); users with modify permissions: Set(yarn, hdfs)
15/11/12 14:43:04 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/11/12 14:43:04 INFO Remoting: Starting remoting
15/11/12 14:43:05 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.2.15:58025]
15/11/12 14:43:05 INFO util.Utils: Successfully started service 'sparkDriver' on port 58025.
15/11/12 14:43:05 INFO spark.SparkEnv: Registering MapOutputTracker
15/11/12 14:43:05 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:05 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:05 INFO spark.SparkEnv: Registering BlockManagerMaster
15/11/12 14:43:05 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/hdfs/appcache/application_1447159372353_0025/blockmgr-61be1a69-fda5-43fe-911a-f234aa1c992f
15/11/12 14:43:05 INFO storage.MemoryStore: MemoryStore started with capacity 245.7 MB
15/11/12 14:43:05 INFO spark.HttpFileServer: HTTP File server directory is /hadoop/yarn/local/usercache/hdfs/appcache/application_1447159372353_0025/httpd-21cb5331-3f66-4b38-a472-533e38b637ac
15/11/12 14:43:05 INFO spark.HttpServer: Starting HTTP Server
15/11/12 14:43:05 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/11/12 14:43:05 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:54144
15/11/12 14:43:05 INFO util.Utils: Successfully started service 'HTTP file server' on port 54144.
15/11/12 14:43:05 INFO spark.SparkEnv: Registering OutputCommitCoordinator
15/11/12 14:43:05 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
15/11/12 14:43:13 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/11/12 14:43:15 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/11/12 14:43:15 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:38787
15/11/12 14:43:15 INFO util.Utils: Successfully started service 'SparkUI' on port 38787.
15/11/12 14:43:15 INFO ui.SparkUI: Started SparkUI at http://10.0.2.15:38787
15/11/12 14:43:15 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
15/11/12 14:43:15 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:15 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:15 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:15 INFO cluster.YarnExtensionServices: Spark YARN services: org.apache.spark.deploy.yarn.history.YarnHistoryService
15/11/12 14:43:16 INFO impl.TimelineClientImpl: Timeline service address: http://sandbox.hortonworks.com:8188/ws/v1/timeline/
15/11/12 14:43:16 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:16 INFO history.YarnHistoryService: Starting Dequeue service for AppId application_1447159372353_0025
15/11/12 14:43:16 INFO history.YarnHistoryService: Service History Service in state History Service: STARTED endpoint=http://sandbox.hortonworks.com:8188/ws/v1/timeline/; bonded to ATS=true; listening=true; batchSize=3; flush count=0; current queue size=0; total number queued=0, processed=0; post failures=0;
15/11/12 14:43:16 INFO cluster.YarnExtensionServices: Service org.apache.spark.deploy.yarn.history.YarnHistoryService started
15/11/12 14:43:16 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:16 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:16 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:16 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52725.
15/11/12 14:43:16 INFO netty.NettyBlockTransferService: Server created on 52725
15/11/12 14:43:16 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/11/12 14:43:16 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:52725 with 245.7 MB RAM, BlockManagerId(driver, 10.0.2.15, 52725)
15/11/12 14:43:16 INFO storage.BlockManagerMaster: Registered BlockManager
15/11/12 14:43:17 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:17 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:17 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka://sparkDriver/user/YarnAM#-1390125051])
15/11/12 14:43:17 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:17 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:17 INFO client.RMProxy: Connecting to ResourceManager at sandbox.hortonworks.com/10.0.2.15:8030
15/11/12 14:43:17 INFO yarn.YarnRMClient: Registering the ApplicationMaster
15/11/12 14:43:17 INFO yarn.YarnAllocator: Will request 1 executor containers, each with 1 cores and 896 MB memory including 384 MB overhead
15/11/12 14:43:17 INFO yarn.YarnAllocator: Container request (host: Any, capability: <memory:896, vCores:1>)
15/11/12 14:43:17 INFO yarn.ApplicationMaster: Started progress reporter thread - sleep time : 5000
15/11/12 14:43:22 INFO impl.AMRMClientImpl: Received new token for : sandbox.hortonworks.com:45454
15/11/12 14:43:23 INFO yarn.YarnAllocator: Launching container container_e05_1447159372353_0025_01_000002 for on host sandbox.hortonworks.com
15/11/12 14:43:23 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.0.2.15:58025/user/CoarseGrainedScheduler,  executorHostname: sandbox.hortonworks.com
15/11/12 14:43:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/11/12 14:43:23 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
15/11/12 14:43:23 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
15/11/12 14:43:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/11/12 14:43:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/11/12 14:43:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource { scheme: "hdfs" host: "sandbox.hortonworks.com" port: 8020 file: "/user/hdfs/.sparkStaging/application_1447159372353_0025/Spark-Yarn-Hortonworks-1.4.1.2.3.2.0-2950.jar" } size: 17889 timestamp: 1447339377047 type: FILE visibility: PRIVATE, __spark__.jar -> resource { scheme: "hdfs" host: "sandbox.hortonworks.com" port: 8020 file: "/user/hdfs/.sparkStaging/application_1447159372353_0025/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar" } size: 167557539 timestamp: 1447339376776 type: FILE visibility: PRIVATE)
15/11/12 14:43:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.3.2.0-2950/hadoop/lib/hadoop-lzo-0.6.0.2.3.2.0-2950.jar:/etc/hadoop/conf/secure, SPARK_LOG_URL_STDERR -> http://sandbox.hortonworks.com:8042/node/containerlogs/container_e05_1447159372353_0025_01_000002/hdfs/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1447159372353_0025, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 167557539,17889, SPARK_USER -> hdfs, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1447339376776,1447339377047, SPARK_LOG_URL_STDOUT -> http://sandbox.hortonworks.com:8042/node/containerlogs/container_e05_1447159372353_0025_01_000002/hdfs/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://sandbox.hortonworks.com:8020/user/hdfs/.sparkStaging/application_1447159372353_0025/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar#__spark__.jar,hdfs://sandbox.hortonworks.com:8020/user/hdfs/.sparkStaging/application_1447159372353_0025/Spark-Yarn-Hortonworks-1.4.1.2.3.2.0-2950.jar#__app__.jar)
15/11/12 14:43:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List({{JAVA_HOME}}/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms512m, -Xmx512m, -Djava.io.tmpdir={{PWD}}/tmp, '-Dspark.driver.port=58025', '-Dspark.history.ui.port=18080', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.0.2.15:58025/user/CoarseGrainedScheduler, --executor-id, 1, --hostname, sandbox.hortonworks.com, --cores, 1, --app-id, application_1447159372353_0025, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/11/12 14:43:23 INFO impl.ContainerManagementProtocolProxy: Opening proxy : sandbox.hortonworks.com:45454
15/11/12 14:43:29 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. sandbox.hortonworks.com:53323
15/11/12 14:43:30 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@sandbox.hortonworks.com:49167/user/Executor#36466835]) with ID 1
15/11/12 14:43:30 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
15/11/12 14:43:30 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
15/11/12 14:43:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager sandbox.hortonworks.com:57213 with 265.4 MB RAM, BlockManagerId(1, sandbox.hortonworks.com, 57213)
15/11/12 14:43:30 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:31 INFO storage.MemoryStore: ensureFreeSpace(218024) called with curMem=0, maxMem=257635123
15/11/12 14:43:31 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 212.9 KB, free 245.5 MB)
15/11/12 14:43:31 INFO storage.MemoryStore: ensureFreeSpace(25919) called with curMem=218024, maxMem=257635123
15/11/12 14:43:31 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 245.5 MB)
15/11/12 14:43:31 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:52725 (size: 25.3 KB, free: 245.7 MB)
15/11/12 14:43:31 INFO spark.SparkContext: Created broadcast 0 from textFile at Sample.scala:36
15/11/12 14:43:33 INFO parquet.ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/11/12 14:43:33 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/11/12 14:43:33 INFO sources.DefaultWriterContainer: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
15/11/12 14:43:33 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/11/12 14:43:33 INFO mapred.FileInputFormat: Total input paths to process : 1
15/11/12 14:43:34 INFO spark.SparkContext: Starting job: parquet at Sample.scala:65
15/11/12 14:43:34 INFO scheduler.DAGScheduler: Got job 0 (parquet at Sample.scala:65) with 2 output partitions (allowLocal=false)
15/11/12 14:43:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 0(parquet at Sample.scala:65)
15/11/12 14:43:34 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/11/12 14:43:34 INFO scheduler.DAGScheduler: Missing parents: List()
15/11/12 14:43:34 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at rddToDataFrameHolder at Sample.scala:43), which has no missing parents
15/11/12 14:43:34 INFO storage.MemoryStore: ensureFreeSpace(81840) called with curMem=243943, maxMem=257635123
15/11/12 14:43:34 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 79.9 KB, free 245.4 MB)
15/11/12 14:43:34 INFO storage.MemoryStore: ensureFreeSpace(30152) called with curMem=325783, maxMem=257635123
15/11/12 14:43:34 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.4 KB, free 245.4 MB)
15/11/12 14:43:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:52725 (size: 29.4 KB, free: 245.6 MB)
15/11/12 14:43:34 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/11/12 14:43:34 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at rddToDataFrameHolder at Sample.scala:43)
15/11/12 14:43:34 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 2 tasks
15/11/12 14:43:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, sandbox.hortonworks.com, NODE_LOCAL, 1436 bytes)
15/11/12 14:43:35 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on sandbox.hortonworks.com:57213 (size: 29.4 KB, free: 265.4 MB)
15/11/12 14:43:36 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on sandbox.hortonworks.com:57213 (size: 25.3 KB, free: 265.4 MB)
15/11/12 14:43:38 INFO storage.BlockManagerInfo: Added rdd_2_0 in memory on sandbox.hortonworks.com:57213 (size: 419.4 KB, free: 264.9 MB)
15/11/12 14:43:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, sandbox.hortonworks.com, NODE_LOCAL, 1436 bytes)
15/11/12 14:43:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7830 ms on sandbox.hortonworks.com (1/2)
15/11/12 14:43:42 INFO storage.BlockManagerInfo: Added rdd_2_1 in memory on sandbox.hortonworks.com:57213 (size: 379.4 KB, free: 264.6 MB)
15/11/12 14:43:43 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1814 ms on sandbox.hortonworks.com (2/2)
15/11/12 14:43:43 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at Sample.scala:65) finished in 9.697 s
15/11/12 14:43:43 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/11/12 14:43:43 INFO scheduler.DAGScheduler: Job 0 finished: parquet at Sample.scala:65, took 9.939866 s
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/11/12 14:43:45 INFO sources.DefaultWriterContainer: Job job_201511121443_0000 committed.
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(216120) called with curMem=355935, maxMem=257635123
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.1 KB, free 245.2 MB)
15/11/12 14:43:46 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:52725 in memory (size: 29.4 KB, free: 245.7 MB)
15/11/12 14:43:46 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on sandbox.hortonworks.com:57213 in memory (size: 29.4 KB, free: 264.6 MB)
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(25919) called with curMem=460063, maxMem=257635123
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.3 KB, free 245.2 MB)
15/11/12 14:43:46 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:52725 (size: 25.3 KB, free: 245.7 MB)
15/11/12 14:43:46 INFO spark.SparkContext: Created broadcast 2 from count at Sample.scala:77
15/11/12 14:43:46 INFO execution.Exchange: Using SparkSqlSerializer2.
15/11/12 14:43:46 INFO spark.SparkContext: Starting job: count at Sample.scala:77
15/11/12 14:43:46 INFO parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Registering RDD 11 (count at Sample.scala:77)
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Got job 1 (count at Sample.scala:77) with 1 output partitions (allowLocal=false)
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 2(count at Sample.scala:77)
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[11] at count at Sample.scala:77), which has no missing parents
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(8864) called with curMem=485982, maxMem=257635123
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.7 KB, free 245.2 MB)
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(4404) called with curMem=494846, maxMem=257635123
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.3 KB, free 245.2 MB)
15/11/12 14:43:46 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:52725 (size: 4.3 KB, free: 245.6 MB)
15/11/12 14:43:46 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/11/12 14:43:46 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[11] at count at Sample.scala:77)
15/11/12 14:43:46 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 2 tasks
15/11/12 14:43:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, sandbox.hortonworks.com, NODE_LOCAL, 1707 bytes)
15/11/12 14:43:46 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on sandbox.hortonworks.com:57213 (size: 4.3 KB, free: 264.6 MB)
15/11/12 14:43:46 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on sandbox.hortonworks.com:57213 (size: 25.3 KB, free: 264.6 MB)
15/11/12 14:43:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, sandbox.hortonworks.com, NODE_LOCAL, 1704 bytes)
15/11/12 14:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 894 ms on sandbox.hortonworks.com (1/2)
15/11/12 14:43:47 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 81 ms on sandbox.hortonworks.com (2/2)
15/11/12 14:43:47 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (count at Sample.scala:77) finished in 0.957 s
15/11/12 14:43:47 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/11/12 14:43:47 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/11/12 14:43:47 INFO scheduler.DAGScheduler: running: Set()
15/11/12 14:43:47 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
15/11/12 14:43:47 INFO scheduler.DAGScheduler: failed: Set()
15/11/12 14:43:47 INFO scheduler.DAGScheduler: Missing parents for ResultStage 2: List()
15/11/12 14:43:47 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at count at Sample.scala:77), which is now runnable
15/11/12 14:43:47 INFO storage.MemoryStore: ensureFreeSpace(10192) called with curMem=499250, maxMem=257635123
15/11/12 14:43:47 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.0 KB, free 245.2 MB)
15/11/12 14:43:47 INFO storage.MemoryStore: ensureFreeSpace(5016) called with curMem=509442, maxMem=257635123
15/11/12 14:43:47 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.9 KB, free 245.2 MB)
15/11/12 14:43:47 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:52725 (size: 4.9 KB, free: 245.6 MB)
15/11/12 14:43:47 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:874
15/11/12 14:43:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at count at Sample.scala:77)
15/11/12 14:43:47 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 1 tasks
15/11/12 14:43:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, sandbox.hortonworks.com, PROCESS_LOCAL, 1165 bytes)
15/11/12 14:43:47 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on sandbox.hortonworks.com:57213 (size: 4.9 KB, free: 264.6 MB)
15/11/12 14:43:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to sandbox.hortonworks.com:49167
15/11/12 14:43:47 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 161 bytes
15/11/12 14:43:47 INFO scheduler.DAGScheduler: ResultStage 2 (count at Sample.scala:77) finished in 0.261 s
15/11/12 14:43:47 INFO scheduler.DAGScheduler: Job 1 finished: count at Sample.scala:77, took 1.466617 s
15/11/12 14:43:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 260 ms on sandbox.hortonworks.com (1/1)
15/11/12 14:43:47 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/11/12 14:43:47 INFO storage.MemoryStore: ensureFreeSpace(216160) called with curMem=514458, maxMem=257635123
15/11/12 14:43:47 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 211.1 KB, free 245.0 MB)
15/11/12 14:43:47 INFO storage.MemoryStore: ensureFreeSpace(25919) called with curMem=730618, maxMem=257635123
15/11/12 14:43:47 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.3 KB, free 245.0 MB)
15/11/12 14:43:47 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:52725 (size: 25.3 KB, free: 245.6 MB)
15/11/12 14:43:47 INFO spark.SparkContext: Created broadcast 5 from count at Sample.scala:78
15/11/12 14:43:47 INFO execution.Exchange: Using SparkSqlSerializer2.
15/11/12 14:43:47 INFO spark.SparkContext: Starting job: count at Sample.scala:78
15/11/12 14:43:47 INFO parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Registering RDD 21 (count at Sample.scala:78)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Got job 2 (count at Sample.scala:78) with 1 output partitions (allowLocal=false)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 4(count at Sample.scala:78)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[21] at count at Sample.scala:78), which has no missing parents
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(8864) called with curMem=756537, maxMem=257635123
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.7 KB, free 245.0 MB)
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(4404) called with curMem=765401, maxMem=257635123
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.3 KB, free 245.0 MB)
15/11/12 14:43:48 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:52725 (size: 4.3 KB, free: 245.6 MB)
15/11/12 14:43:48 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:874
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[21] at count at Sample.scala:78)
15/11/12 14:43:48 INFO cluster.YarnClusterScheduler: Adding task set 3.0 with 2 tasks
15/11/12 14:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5, sandbox.hortonworks.com, NODE_LOCAL, 1707 bytes)
15/11/12 14:43:48 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on sandbox.hortonworks.com:57213 (size: 4.3 KB, free: 264.6 MB)
15/11/12 14:43:48 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on sandbox.hortonworks.com:57213 (size: 25.3 KB, free: 264.5 MB)
15/11/12 14:43:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6, sandbox.hortonworks.com, NODE_LOCAL, 1704 bytes)
15/11/12 14:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 144 ms on sandbox.hortonworks.com (1/2)
15/11/12 14:43:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 61 ms on sandbox.hortonworks.com (2/2)
15/11/12 14:43:48 INFO cluster.YarnClusterScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/11/12 14:43:48 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (count at Sample.scala:78) finished in 0.204 s
15/11/12 14:43:48 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/11/12 14:43:48 INFO scheduler.DAGScheduler: running: Set()
15/11/12 14:43:48 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: failed: Set()
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Missing parents for ResultStage 4: List()
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at count at Sample.scala:78), which is now runnable
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(10192) called with curMem=769805, maxMem=257635123
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.0 KB, free 245.0 MB)
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(5033) called with curMem=779997, maxMem=257635123
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.9 KB, free 245.0 MB)
15/11/12 14:43:48 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:52725 (size: 4.9 KB, free: 245.6 MB)
15/11/12 14:43:48 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at count at Sample.scala:78)
15/11/12 14:43:48 INFO cluster.YarnClusterScheduler: Adding task set 4.0 with 1 tasks
15/11/12 14:43:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7, sandbox.hortonworks.com, PROCESS_LOCAL, 1165 bytes)
15/11/12 14:43:48 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on sandbox.hortonworks.com:57213 (size: 4.9 KB, free: 264.5 MB)
15/11/12 14:43:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to sandbox.hortonworks.com:49167
15/11/12 14:43:48 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 161 bytes
15/11/12 14:43:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 60 ms on sandbox.hortonworks.com (1/1)
15/11/12 14:43:48 INFO scheduler.DAGScheduler: ResultStage 4 (count at Sample.scala:78) finished in 0.057 s
15/11/12 14:43:48 INFO cluster.YarnClusterScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Job 2 finished: count at Sample.scala:78, took 0.308734 s
15/11/12 14:43:48 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
15/11/12 14:43:48 INFO spark.SparkContext: Invoking stop() from shutdown hook
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/11/12 14:43:48 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/11/12 14:43:48 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.2.15:38787
15/11/12 14:43:48 INFO scheduler.DAGScheduler: Stopping DAGScheduler
15/11/12 14:43:48 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
15/11/12 14:43:48 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down
15/11/12 14:43:48 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. sandbox.hortonworks.com:49167
15/11/12 14:43:48 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/11/12 14:43:48 INFO storage.MemoryStore: MemoryStore cleared
15/11/12 14:43:48 INFO storage.BlockManager: BlockManager stopped
15/11/12 14:43:48 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
15/11/12 14:43:48 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/11/12 14:43:48 INFO spark.SparkContext: Successfully stopped SparkContext
15/11/12 14:43:48 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
15/11/12 14:43:48 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/11/12 14:43:48 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/11/12 14:43:48 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
15/11/12 14:43:48 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/11/12 14:43:48 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1447159372353_0025
15/11/12 14:43:48 INFO util.Utils: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Thu Nov 12 14:43:50 +0000 2015
LogLength:1195
Log Contents:
#RAM# : ******************************* v 1.0 ***************************************
#RAM# : 1. Loading file
#RAM# : 2. Split each row, and cache it!
#RAM# : 3. Start building the dataframe
#RAM# : 4. Printing dataframe schema
root
 |-- id: string (nullable = true)
 |-- cod: string (nullable = true)
 |-- date: string (nullable = true)
 |-- type1: string (nullable = true)
 |-- type2: string (nullable = true)
 |-- internalId: string (nullable = true)
 |-- category: string (nullable = true)

#RAM# : 5.0 Remove existing parquet file
#RAM# : 5.1 Write DF in parquet
#RAM# : 6. Read DF in parquet
#RAM# : 7. Create Table <dataParquetTable>
#RAM# : 8. Filter DF in parquet
#RAM# : 9. Final count: 436
#RAM# : ******************************* done! ***************************************
Nov 12, 2015 2:43:44 PM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Nov 12, 2015 2:43:45 PM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Nov 12, 2015 2:43:45 PM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Nov 12, 2015 2:43:45 PM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
End of LogType:stdout



Container: container_e05_1447159372353_0025_01_000002 on sandbox.hortonworks.com_45454
========================================================================================
LogType:stderr
Log Upload Time:Thu Nov 12 14:43:50 +0000 2015
LogLength:26205
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/usercache/hdfs/filecache/73/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/11/12 14:43:24 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
15/11/12 14:43:26 INFO spark.SecurityManager: Changing view acls to: yarn,hdfs
15/11/12 14:43:26 INFO spark.SecurityManager: Changing modify acls to: yarn,hdfs
15/11/12 14:43:26 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hdfs); users with modify permissions: Set(yarn, hdfs)
15/11/12 14:43:27 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/11/12 14:43:28 INFO Remoting: Starting remoting
15/11/12 14:43:28 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@sandbox.hortonworks.com:53323]
15/11/12 14:43:28 INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 53323.
15/11/12 14:43:29 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:29 INFO spark.SecurityManager: Changing view acls to: yarn,hdfs
15/11/12 14:43:29 INFO spark.SecurityManager: Changing modify acls to: yarn,hdfs
15/11/12 14:43:29 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, hdfs); users with modify permissions: Set(yarn, hdfs)
15/11/12 14:43:29 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/11/12 14:43:29 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/11/12 14:43:29 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/11/12 14:43:29 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/11/12 14:43:29 INFO Remoting: Starting remoting
15/11/12 14:43:29 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@sandbox.hortonworks.com:49167]
15/11/12 14:43:29 INFO util.Utils: Successfully started service 'sparkExecutor' on port 49167.
15/11/12 14:43:29 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:29 WARN spark.SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/11/12 14:43:29 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/hdfs/appcache/application_1447159372353_0025/blockmgr-bb948d3d-070e-488f-84bb-5c8f9a7c9a59
15/11/12 14:43:29 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
15/11/12 14:43:29 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@10.0.2.15:58025/user/CoarseGrainedScheduler
15/11/12 14:43:30 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
15/11/12 14:43:30 INFO executor.Executor: Starting executor ID 1 on host sandbox.hortonworks.com
15/11/12 14:43:30 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57213.
15/11/12 14:43:30 INFO netty.NettyBlockTransferService: Server created on 57213
15/11/12 14:43:30 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/11/12 14:43:30 INFO storage.BlockManagerMaster: Registered BlockManager
15/11/12 14:43:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
15/11/12 14:43:34 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/12 14:43:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
15/11/12 14:43:35 INFO storage.MemoryStore: ensureFreeSpace(30152) called with curMem=0, maxMem=278302556
15/11/12 14:43:35 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.4 KB, free 265.4 MB)
15/11/12 14:43:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 456 ms
15/11/12 14:43:35 INFO storage.MemoryStore: ensureFreeSpace(81840) called with curMem=30152, maxMem=278302556
15/11/12 14:43:35 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 79.9 KB, free 265.3 MB)
15/11/12 14:43:36 INFO spark.CacheManager: Partition rdd_2_0 not found, computing it
15/11/12 14:43:36 INFO rdd.HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/acct.txt:0+50614
15/11/12 14:43:36 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
15/11/12 14:43:36 INFO storage.MemoryStore: ensureFreeSpace(25919) called with curMem=111992, maxMem=278302556
15/11/12 14:43:36 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 265.3 MB)
15/11/12 14:43:36 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 15 ms
15/11/12 14:43:37 INFO storage.MemoryStore: ensureFreeSpace(400296) called with curMem=137911, maxMem=278302556
15/11/12 14:43:37 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 390.9 KB, free 264.9 MB)
15/11/12 14:43:38 INFO storage.MemoryStore: ensureFreeSpace(429512) called with curMem=538207, maxMem=278302556
15/11/12 14:43:38 INFO storage.MemoryStore: Block rdd_2_0 stored as values in memory (estimated size 419.4 KB, free 264.5 MB)
15/11/12 14:43:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/11/12 14:43:39 INFO sources.DefaultWriterContainer: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
15/11/12 14:43:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/11/12 14:43:40 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
15/11/12 14:43:40 INFO compress.CodecPool: Got brand-new compressor [.gz]
15/11/12 14:43:40 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 43,060,553
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 5,491B for [id] BINARY: 1,001 values, 14,020B raw, 5,443B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 951B for [cod] BINARY: 1,001 values, 892B raw, 915B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 99 entries, 792B raw, 99B comp}
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 1,333B for [date] BINARY: 1,001 values, 1,270B raw, 1,293B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 599 entries, 7,184B raw, 599B comp}
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 346B for [type1] BINARY: 1,001 values, 388B raw, 311B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 33B raw, 5B comp}
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 1,202B for [type2] BINARY: 1,001 values, 1,144B raw, 1,167B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 287 entries, 1,925B raw, 287B comp}
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 6,889B for [internalId] BINARY: 1,001 values, 17,380B raw, 6,856B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/11/12 14:43:40 INFO hadoop.ColumnChunkPageWriteStore: written 294B for [category] BINARY: 1,001 values, 263B raw, 257B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 31B raw, 4B comp}
15/11/12 14:43:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_201511121443_0000_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/dataParquet/_temporary/0/task_201511121443_0000_m_000000
15/11/12 14:43:42 INFO mapred.SparkHadoopMapRedUtil: attempt_201511121443_0000_m_000000_0: Committed
15/11/12 14:43:42 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2386 bytes result sent to driver
15/11/12 14:43:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
15/11/12 14:43:42 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/12 14:43:42 INFO spark.CacheManager: Partition rdd_2_1 not found, computing it
15/11/12 14:43:42 INFO rdd.HadoopRDD: Input split: hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/acct.txt:50614+50615
15/11/12 14:43:42 INFO storage.MemoryStore: ensureFreeSpace(388536) called with curMem=967719, maxMem=278302556
15/11/12 14:43:42 INFO storage.MemoryStore: Block rdd_2_1 stored as values in memory (estimated size 379.4 KB, free 264.1 MB)
15/11/12 14:43:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/11/12 14:43:42 INFO sources.DefaultWriterContainer: Using user defined output committer class parquet.hadoop.ParquetOutputCommitter
15/11/12 14:43:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/11/12 14:43:42 INFO codec.CodecConfig: Compression: GZIP
15/11/12 14:43:42 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
15/11/12 14:43:42 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
15/11/12 14:43:42 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
15/11/12 14:43:42 INFO hadoop.ParquetOutputFormat: Dictionary is on
15/11/12 14:43:42 INFO hadoop.ParquetOutputFormat: Validation is off
15/11/12 14:43:42 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
15/11/12 14:43:42 INFO compress.CodecPool: Got brand-new compressor [.gz]
15/11/12 14:43:42 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 43,060,562
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 5,506B for [id] BINARY: 999 values, 13,993B raw, 5,457B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 944B for [cod] BINARY: 999 values, 885B raw, 908B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 104 entries, 832B raw, 104B comp}
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 1,327B for [date] BINARY: 999 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 592 entries, 7,104B raw, 592B comp}
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 261B for [type1] BINARY: 999 values, 261B raw, 229B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 4 entries, 24B raw, 4B comp}
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 1,189B for [type2] BINARY: 999 values, 1,135B raw, 1,158B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 295 entries, 1,970B raw, 295B comp}
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 6,902B for [internalId] BINARY: 999 values, 17,458B raw, 6,858B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/11/12 14:43:43 INFO hadoop.ColumnChunkPageWriteStore: written 288B for [category] BINARY: 999 values, 260B raw, 254B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 21B raw, 3B comp}
15/11/12 14:43:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_201511121443_0000_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/dataParquet/_temporary/0/task_201511121443_0000_m_000001
15/11/12 14:43:43 INFO mapred.SparkHadoopMapRedUtil: attempt_201511121443_0000_m_000001_0: Committed
15/11/12 14:43:43 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2386 bytes result sent to driver
15/11/12 14:43:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
15/11/12 14:43:46 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
15/11/12 14:43:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(4404) called with curMem=1244263, maxMem=278302556
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.3 KB, free 264.2 MB)
15/11/12 14:43:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 42 ms
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(8864) called with curMem=1248667, maxMem=278302556
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.7 KB, free 264.2 MB)
15/11/12 14:43:46 INFO parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/dataParquet/part-r-00001-233fb9d7-b1a9-45d2-b918-c158e18cc7ff.gz.parquet start: 0 end: 20202 length: 20202 hosts: [] requestedSchema: message root {
  optional binary category (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"cod","type":"string","nullable":true,"metadata":{}},{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"type1","type":"string","nullable":true,"metadata":{}},{"name":"type2","type":"string","nullable":true,"metadata":{}},{"name":"internalId","type":"string","nullable":true,"metadata":{}},{"name":"category","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"category","type":"string","nullable":true,"metadata":{}}]}}}
15/11/12 14:43:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(25919) called with curMem=1257531, maxMem=278302556
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.3 KB, free 264.2 MB)
15/11/12 14:43:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 33 ms
15/11/12 14:43:46 INFO storage.MemoryStore: ensureFreeSpace(418080) called with curMem=1283450, maxMem=278302556
15/11/12 14:43:46 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 408.3 KB, free 263.8 MB)
15/11/12 14:43:47 WARN hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/11/12 14:43:47 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 999 records.
15/11/12 14:43:47 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
15/11/12 14:43:47 INFO compress.CodecPool: Got brand-new decompressor [.gz]
15/11/12 14:43:47 INFO hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 999
15/11/12 14:43:47 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 2024 bytes result sent to driver
15/11/12 14:43:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
15/11/12 14:43:47 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
15/11/12 14:43:47 INFO parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/dataParquet/part-r-00000-233fb9d7-b1a9-45d2-b918-c158e18cc7ff.gz.parquet start: 0 end: 20327 length: 20327 hosts: [] requestedSchema: message root {
  optional binary category (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"cod","type":"string","nullable":true,"metadata":{}},{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"type1","type":"string","nullable":true,"metadata":{}},{"name":"type2","type":"string","nullable":true,"metadata":{}},{"name":"internalId","type":"string","nullable":true,"metadata":{}},{"name":"category","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"category","type":"string","nullable":true,"metadata":{}}]}}}
15/11/12 14:43:47 WARN hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/11/12 14:43:47 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1001 records.
15/11/12 14:43:47 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
15/11/12 14:43:47 INFO hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 1001
15/11/12 14:43:47 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 2024 bytes result sent to driver
15/11/12 14:43:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
15/11/12 14:43:47 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 4)
15/11/12 14:43:47 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
15/11/12 14:43:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
15/11/12 14:43:47 INFO storage.MemoryStore: ensureFreeSpace(5016) called with curMem=1701530, maxMem=278302556
15/11/12 14:43:47 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.9 KB, free 263.8 MB)
15/11/12 14:43:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 21 ms
15/11/12 14:43:47 INFO storage.MemoryStore: ensureFreeSpace(10192) called with curMem=1706546, maxMem=278302556
15/11/12 14:43:47 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.0 KB, free 263.8 MB)
15/11/12 14:43:47 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
15/11/12 14:43:47 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = AkkaRpcEndpointRef(Actor[akka.tcp://sparkDriver@10.0.2.15:58025/user/MapOutputTracker#672075403])
15/11/12 14:43:47 INFO spark.MapOutputTrackerWorker: Got the output locations
15/11/12 14:43:47 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/11/12 14:43:47 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/11/12 14:43:47 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 4). 1940 bytes result sent to driver
15/11/12 14:43:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
15/11/12 14:43:48 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 5)
15/11/12 14:43:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(4404) called with curMem=1716738, maxMem=278302556
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.3 KB, free 263.8 MB)
15/11/12 14:43:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 14 ms
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(8864) called with curMem=1721142, maxMem=278302556
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.7 KB, free 263.8 MB)
15/11/12 14:43:48 INFO parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/dataParquet/part-r-00001-233fb9d7-b1a9-45d2-b918-c158e18cc7ff.gz.parquet start: 0 end: 20202 length: 20202 hosts: [] requestedSchema: message root {
  optional binary category (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"cod","type":"string","nullable":true,"metadata":{}},{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"type1","type":"string","nullable":true,"metadata":{}},{"name":"type2","type":"string","nullable":true,"metadata":{}},{"name":"internalId","type":"string","nullable":true,"metadata":{}},{"name":"category","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"category","type":"string","nullable":true,"metadata":{}}]}}}
15/11/12 14:43:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(25919) called with curMem=1730006, maxMem=278302556
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 25.3 KB, free 263.7 MB)
15/11/12 14:43:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 17 ms
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(418080) called with curMem=1755925, maxMem=278302556
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 408.3 KB, free 263.3 MB)
15/11/12 14:43:48 WARN hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/11/12 14:43:48 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 999 records.
15/11/12 14:43:48 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
15/11/12 14:43:48 INFO hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 999
15/11/12 14:43:48 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 5). 2024 bytes result sent to driver
15/11/12 14:43:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
15/11/12 14:43:48 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 6)
15/11/12 14:43:48 INFO parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/demo/data/Customer/dataParquet/part-r-00000-233fb9d7-b1a9-45d2-b918-c158e18cc7ff.gz.parquet start: 0 end: 20327 length: 20327 hosts: [] requestedSchema: message root {
  optional binary category (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"cod","type":"string","nullable":true,"metadata":{}},{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"type1","type":"string","nullable":true,"metadata":{}},{"name":"type2","type":"string","nullable":true,"metadata":{}},{"name":"internalId","type":"string","nullable":true,"metadata":{}},{"name":"category","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"category","type":"string","nullable":true,"metadata":{}}]}}}
15/11/12 14:43:48 WARN hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/11/12 14:43:48 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1001 records.
15/11/12 14:43:48 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
15/11/12 14:43:48 INFO hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 1001
15/11/12 14:43:48 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 6). 2024 bytes result sent to driver
15/11/12 14:43:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
15/11/12 14:43:48 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 7)
15/11/12 14:43:48 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
15/11/12 14:43:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(5033) called with curMem=2174005, maxMem=278302556
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.9 KB, free 263.3 MB)
15/11/12 14:43:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 15 ms
15/11/12 14:43:48 INFO storage.MemoryStore: ensureFreeSpace(10192) called with curMem=2179038, maxMem=278302556
15/11/12 14:43:48 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.0 KB, free 263.3 MB)
15/11/12 14:43:48 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/11/12 14:43:48 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = AkkaRpcEndpointRef(Actor[akka.tcp://sparkDriver@10.0.2.15:58025/user/MapOutputTracker#672075403])
15/11/12 14:43:48 INFO spark.MapOutputTrackerWorker: Got the output locations
15/11/12 14:43:48 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
15/11/12 14:43:48 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/11/12 14:43:48 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 7). 1940 bytes result sent to driver
15/11/12 14:43:48 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
15/11/12 14:43:48 INFO storage.MemoryStore: MemoryStore cleared
15/11/12 14:43:48 INFO storage.BlockManager: BlockManager stopped
15/11/12 14:43:48 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/11/12 14:43:48 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/11/12 14:43:48 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/11/12 14:43:48 INFO util.Utils: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Thu Nov 12 14:43:50 +0000 2015
LogLength:0
Log Contents:
End of LogType:stdout

